# Communication Centralizer

An automated system that consolidates messages from multiple platforms (Email, LinkedIn, Handshake, Slack, Discord, etc.) into a single Google Sheet, then uses NLP to detect interview requests and auto-schedules them with Calendly and Google Calendar.

## Features

- Collects messages from:
  - Emails (Gmail/Outlook)
  - LinkedIn DMs (via PhantomBuster)
  - Handshake Messages (via web automation)
  - Slack and Discord
- Stores data in Google Sheets
- Identifies interview requests using spaCy (with LLM integration)
- Auto-schedules interviews with Calendly/Google Calendar

## Requirements

- **Docker** and **Docker Compose** (automatically installed by `setup.sh`)
- **Google Cloud Platform** account (for Sheets/Calendar)
- **PhantomBuster** account (for LinkedIn automation)
- **Calendly** account (for scheduling)
- **Slack** and **Discord** bot tokens (optional connectors)
- **AWS** account (if using the included CI/CD to deploy to an EC2 instance, but you can skip AWS if you just need local usage)

## Quick Setup

1. **Clone the repository**:
   ```bash
   git clone https://github.com/your-org/comm-centralizer.git
   cd comm-centralizer
   ```
2. **Run the setup script**:
   ```bash
   ./setup.sh
   ```
   This will:
   - Install Docker and Docker Compose (if missing)
   - Build the Docker development container with Python 3.11
   - Configure Docker-based pre-commit hooks
   - Prepare necessary directories and configs
3. **Add credentials** to `config/credentials/` (see that folder's README for details).
4. **Set environment variables** in the `.env` file (you can copy `.env.example` as a starting point).
5. **Reload your shell or log out/log in** if prompted (Docker permissions).
6. **Start the app in development mode**:
   ```bash
   make docker-run-dev
   ```

## Docker-Based Development Workflow

All development is done inside Docker containers to ensure consistency:

1. **Start a development shell**:
   ```bash
   make docker-shell
   ```
2. **Run the application in development mode**:

   ```bash
   make docker-run-dev
   ```

3. **Run tests inside Docker**:

   ```bash
   make docker-test
   ```

4. **Run linting and code checks inside Docker**:

   ```bash
   make docker-lint
   make docker-pytype
   ```

5. **Build Docker images**:
   ```bash
   make docker-build-dev    # Development image
   make docker-build-prod   # Production image
   ```

## CI/CD Pipeline (GitHub Actions)

- Automatically tests, builds, and deploys the application when changes are pushed.
- By default:
  - **`develop`** branch â†’ deploys to staging (if configured).
  - **`main`** branch â†’ deploys to production (if configured).
- **Required GitHub Secrets** (if deploying to AWS):
  - `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_REGION`, `STAGING_EC2_IP`, `PRODUCTION_EC2_IP`, `EC2_SSH_PRIVATE_KEY`
- If you're not using AWS, you can ignore this part and just run locally or adapt it to your environment.

## Testing

All tests run inside Docker to ensure consistency:

```bash
# Run all tests in Docker
make docker-test

# Run tests with specific options
make docker-test PYTEST_ARGS="-v tests/component/test_specific.py"
```

### Adding New Tests

- **Credential Tests**: place them in `tests/credentials/` and update `tests/run_credential_tests.py` if needed.
- **Component Tests**: place them in `tests/component/` following normal pytest naming conventions.

## Project Automation with Makefile

- `make help`: Lists all available commands.
- **Common commands**:
  - `make docker-run-dev` - Start the development environment
  - `make docker-shell` - Open a shell in the Docker container
  - `make docker-stop` - Stop containers
  - `make docker-logs` - View logs
  - `make docker-build-dev` - Rebuild development container
  - `make docker-test` - Run tests in Docker
  - `make docker-lint` - Run linting in Docker
  - `make docker-pytype` - Run type checking in Docker
  - `make ci-test` - Run all CI tests

## VS Code Development (Optional)

For the best VS Code experience with Docker-based development:

1. Install the "Remote - Containers" extension
2. Use Command Palette â†’ "Remote-Containers: Attach to Running Container..."
3. Select your development container
4. Edit code directly within the container

Alternatively, you can create a `.devcontainer/devcontainer.json` file with:

```json
{
  "name": "Python 3.11 Development",
  "dockerComposeFile": "../docker-compose.dev.yml",
  "service": "app",
  "workspaceFolder": "/app"
}
```

## Simplified Workflow

1. **Install/Update** everything with `./setup.sh`
2. **Develop** using `make docker-shell` or `make docker-run-dev`
3. **Test** with `make docker-test` and `make docker-lint`
4. **Push changes** to GitHub to trigger automated tests and deployment
5. **Check logs** with `make docker-logs`

## Git Pre-Commit Hooks

The setup automatically configures Git hooks to run pre-commit checks inside Docker, ensuring:

- Code is properly formatted (Black)
- Type checking passes (Pytype with Python 3.11)
- Linting passes (Flake8, Ruff)
- Other checks run as configured in `.pre-commit-config.yaml`

## Project Structure

```bash
comm-centralizer/
â”œâ”€â”€ scripts/
|   â”œâ”€â”€ .dir_structure_cache.json    # Cache file for directory structure
|   â”œâ”€â”€ deploy_to_ec2.sh             # Script for deploying to EC2
|   â”œâ”€â”€ directory_printer.py         # Python script for printing directory structure
|   â”œâ”€â”€ docker-entrypoint.sh         # Docker entrypoint script
|   â”œâ”€â”€ docker-pre-commit.sh         # Pre-commit script for Docker
|   â”œâ”€â”€ ec2_security_setup.sh        # Script for setting up security on EC2
|   â”œâ”€â”€ llm_type_annotations.log     # Log file for type annotations
|   â”œâ”€â”€ readme_update.log            # Log file for updating README
|   â”œâ”€â”€ schedule_job.py              # Script for scheduling jobs
|   â”œâ”€â”€ setup_env_credentials.sh     # Script for setting up environment credentials
|   â”œâ”€â”€ setup_monitoring.sh          # Script for setting up monitoring
|   â””â”€â”€ type_annotate_python_files.py    # Python script for type annotating Python files
â”œâ”€â”€ .ruff_cache/
|   â”œâ”€â”€ .gitignore                  # Git ignore file for cache directory
|   â”œâ”€â”€ CACHEDIR.TAG                # Cache directory tag
|   â””â”€â”€ content/                    # Cache content directory
â”œâ”€â”€ terraform/
|   â””â”€â”€ main.tf                      # Terraform main configuration file
â”œâ”€â”€ config/
|   â”œâ”€â”€ config.py                   # Configuration file
|   â””â”€â”€ credentials/
|       â”œâ”€â”€ .gitkeep                # Git keep file for credentials directory
|       â”œâ”€â”€ README.md               # Credentials README
|       â”œâ”€â”€ gmail_token.json        # Gmail token file
|       â””â”€â”€ google_credentials.json  # Google credentials file
â”œâ”€â”€ src/
|   â”œâ”€â”€ __init__.py                  # Python package initialization file
|   â”œâ”€â”€ main.py                      # Main Python script
|   â”œâ”€â”€ automation/
|   |   â”œâ”€â”€ __init__.py              # Automation package initialization file
|   |   â”œâ”€â”€ puppeteer_scripts/
|   |   |   â”œâ”€â”€ handshake.js        # Puppeteer script for handshake
|   |   |   â”œâ”€â”€ index.ts            # TypeScript index file
|   |   |   â””â”€â”€ utils.js            # JavaScript utility functions
|   |   â””â”€â”€ selenium_scripts/
|   |       â””â”€â”€ utils.py            # Selenium utility functions
|   â”œâ”€â”€ config/
|   |   â””â”€â”€ environment.py          # Environment configuration file
|   â”œâ”€â”€ connectors/
|   |   â”œâ”€â”€ __init__.py              # Connectors package initialization file
|   |   â”œâ”€â”€ discord_connector.py    # Discord connector implementation
|   |   â”œâ”€â”€ email_connector.py      # Email connector implementation
|   |   â”œâ”€â”€ handshake_connector.py  # Handshake connector implementation
|   |   â”œâ”€â”€ linkedin_connector.py   # LinkedIn connector implementation
|   |   â””â”€â”€ slack_connector.py      # Slack connector implementation
|   â”œâ”€â”€ processing/
|   |   â”œâ”€â”€ __init__.py              # Processing package initialization file
|   |   â”œâ”€â”€ message_classifier.py   # Message classifier implementation
|   |   â””â”€â”€ nlp_processor.py        # NLP processor implementation
|   â”œâ”€â”€ scheduling/
|   |   â”œâ”€â”€ __init__.py              # Scheduling package initialization file
|   |   â”œâ”€â”€ calendly.py             # Calendly scheduling implementation
|   |   â””â”€â”€ google_calendar.py      # Google Calendar scheduling implementation
|   â””â”€â”€ storage/
|       â”œâ”€â”€ __init__.py              # Storage package initialization file
|       â””â”€â”€ google_sheets.py        # Google Sheets storage implementation
â”œâ”€â”€ .pytype/
|   â”œâ”€â”€ .gitignore                  # Git ignore file for Pytype
|   â”œâ”€â”€ .ninja_log                  # Ninja log file
|   â”œâ”€â”€ build.ninja                 # Ninja build file
|   â”œâ”€â”€ imports/
|   |   â”œâ”€â”€ default.pyi             # Default Pyi file
|   |   â”œâ”€â”€ tst.imports             # Test imports file
|   |   â””â”€â”€ type_annotate_python_files.imports  # Type annotation imports file
|   â””â”€â”€ pyi/
|       â””â”€â”€ type_annotate_python_files.pyi  # Type annotation Pyi file
â”œâ”€â”€ tests/
|   â”œâ”€â”€ __init__.py                 # Tests package initialization file
|   â”œâ”€â”€ conftest.py                 # Pytest configuration file
|   â”œâ”€â”€ component/
|   |   â”œâ”€â”€ README.md               # Component tests README
|   |   â”œâ”€â”€ test_automation.py      # Automation tests
|   |   â”œâ”€â”€ test_connectors.py      # Connectors tests
|   |   â”œâ”€â”€ test_processing.py      # Processing tests
|   |   â”œâ”€â”€ test_scheduling.py      # Scheduling tests
|   |   â””â”€â”€ test_storage.py         # Storage tests
|   â””â”€â”€ credentials/
|       â”œâ”€â”€ README.md               # Credentials tests README
|       â”œâ”€â”€ test_calendly_credentials.py  # Calendly credentials test
|       â”œâ”€â”€ test_discord_credentials.py   # Discord credentials test
|       â”œâ”€â”€ test_email_credentials.py     # Email credentials test
|       â”œâ”€â”€ test_openai_credentials.py    # OpenAI credentials test
|       â”œâ”€â”€ test_phantombuster_credentials.py  # Phantombuster credentials test
|       â”œâ”€â”€ test_sheets_credentials.py    # Google Sheets credentials test
|       â””â”€â”€ test_slack_credentials.py     # Slack credentials test
â”œâ”€â”€ .eslintrc.js                    # ESLint configuration file
â”œâ”€â”€ .pre-commit-config.yaml         # Pre-commit configuration file
â”œâ”€â”€ Dockerfile                      # Dockerfile for development
â”œâ”€â”€ Dockerfile.prod                 # Dockerfile for production
â”œâ”€â”€ Makefile                        # Makefile for project
â”œâ”€â”€ README.md                       # Project documentation
â”œâ”€â”€ REFERENCES.md                   # References for project
â”œâ”€â”€ TODO.md                         # TODO list for project
â”œâ”€â”€ TODOPROMPTS.txt                 # TODO prompts for project
â”œâ”€â”€ comm_centralizer.log            # Project log file
â”œâ”€â”€ docker-compose.dev.yml          # Docker Compose file for development
â”œâ”€â”€ docker-compose.prod.yml         # Docker Compose file for production
â”œâ”€â”€ docker-compose.yml              # Docker Compose file
â”œâ”€â”€ monkeytype.sqlite3              # Monkeytype SQLite database
â”œâ”€â”€ package.json                    # Node.js package file
â”œâ”€â”€ pyproject.toml                  # Python project configuration file
â”œâ”€â”€ pytest.ini                      # Pytest configuration file
â”œâ”€â”€ setup.sh                        # Setup script
â””â”€â”€ tsconfig.json                   # TypeScript configuration file
```

**That's it!** For most use cases, just run `setup.sh` and use `make docker-run-dev` for development, then rely on the GitHub Actions pipeline to handle testing and production deployment.

## Architecture

```mermaid
graph TD
    subgraph "Input Sources"
        A1["ğŸ“§ Email"]
        A2["ğŸ’¼ LinkedIn"]
        A3["ğŸ¤ Handshake"]
        A4["ğŸ’¬ Slack/Discord"]
    end

    subgraph "Integration Layer"
        B["ğŸ”Œ Connectors"]
    end

    subgraph "Core Processing"
        C["âš™ï¸ Central Processing"]
        D["ğŸ§  NLP/Classification"]
    end

    subgraph "Output Systems"
        E["ğŸ“Š Google Sheets<br/>(Storage)"]
        F["ğŸ“… Calendly<br/>(Scheduling)"]
        G["ğŸ“† Google Calendar"]
    end

    A1 --> B
    A2 --> B
    A3 --> B
    A4 --> B

    B --> C
    C --> D

    D -->|Store Message Data| E
    D -->|Schedule Interview| F
    F --> G

    %% Styling
    classDef sources fill:#f9d5e5,stroke:#333,stroke-width:1px;
    classDef integration fill:#eeeeee,stroke:#333,stroke-width:1px;
    classDef processing fill:#d3e5ef,stroke:#333,stroke-width:1px;
    classDef outputs fill:#e5f5e0,stroke:#333,stroke-width:1px;

    class A1,A2,A3,A4 sources;
    class B integration;
    class C,D processing;
    class E,F,G outputs;
```

# Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Create a pull request

Please ensure you follow our coding standards (linting and type checking should pass).

# Adding New Connectors

To add a new connector, you can follow these steps:

1. Create a new file in the `src/connectors/` directory.
2. Implement the required interface methods (see existing connectors for examples)
3. Add credentials handling in `config/credentials/`
4. Update the main application to use your new connector
5. Add tests in `tests/component/` and `test/credentials`

# Troubleshooting

## Common Issues

- **Docker Permissions**: Ensure you've followed the setup instructions and reloaded your shell. Run `su - $USER` before running docker commands.
- **Authentication failures**: Double-check your credentials in `config/credentials/` and `.env`.
- **Scheduling not working**: Ensure your Calendly and Google Calendar are properly set up.
- **Pre-commit hooks failing**: Run `make docker-lint` to see detailed errors

For more issues, check the logs with `make docker-logs`.
